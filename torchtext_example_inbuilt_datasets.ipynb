{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "\n",
    "from torchtext.vocab import GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use tokenizer in spacy<br />\n",
    "After installing  spacy, additional operation is needed to install language models<br />\n",
    "See https://spacy.io/usage/models for more info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Use English model\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "# create a tokenizer function\n",
    "def tokenizer(text): \n",
    "    text = text.replace(\"<br />\", \" \")\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define attributes in the dataset.<br />\n",
    "Here we use the TREC dataset as example. In the dataset there are only two attributes: text and label<br />\n",
    "We specify steps of preprocessing as parameter. <br />For detailed documentation of all the parameters, see http://torchtext.readthedocs.io/en/latest/data.html#field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequential = True: specify that the data is sequential\n",
    "# include_lengths = True: a list that store the length of each sentence will also be returned\n",
    "# lower = True: lowercase all words\n",
    "# tokenize = tokenizer: use out pre-defined tokenizer to tokenize the text. Default is text.split()\n",
    "# init_token = \"<SOS>\": a <SOS> will be added before the beginning of the text sequence\n",
    "# eos_token = \"<EOS>\": a <EOS> will be added after the end of the text sequence\n",
    "# We did not specify a parameter \"use_vocab\" cause it's already set to True, \n",
    "#        this parameter specify that all words in the text sequence will be converted into indexes\n",
    "#        and we can see the corresponding words to the indexes using TEXT.vocab.stoi\n",
    "TEXT = data.Field(sequential=True, include_lengths=True, lower=True, \n",
    "                  tokenize=tokenizer, init_token='<SOS>', eos_token='<EOS>')\n",
    "\n",
    "# Label is our predict target, and no further operation is needed in this phrase\n",
    "LABEL = data.LabelField()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the in-built dataset TREC, and split it into train and test dataset\n",
    "# Some other in-built dataset contains validation dataset. e.g., SST\n",
    "# In that case you can use the following line of code to get the three subdataset\n",
    "# train, test, val = datasets.SST.splits(TEXT, LABEL)\n",
    "train, test = datasets.TREC.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we print out an Example in the training dataset.<br />\n",
    "You might notice that the data we get is a torchtext.data.example.Example object.<br />\n",
    "To print out the attribute of this object, use \".attribute\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "<torchtext.data.example.Example object at 0x7f1980aa09b0>\n",
      "['how', 'did', 'serfdom', 'develop', 'in', 'and', 'then', 'leave', 'russia', '?']\n",
      "DESC\n"
     ]
    }
   ],
   "source": [
    "for num, batch in enumerate(train):\n",
    "    print(num)\n",
    "    print(batch)\n",
    "    print(batch.text)\n",
    "    print(batch.label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert words to indexes, we need to use the build_vocab() method from torchtext.data.Field object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the in-built word embedding model GloVe\n",
    "# You can use your own word embedding model using torchtext.vocab.Vectors\n",
    "TEXT.build_vocab(train, test, vectors=\"glove.6B.300d\", max_size=30000) \n",
    "\n",
    "# Serialize the label\n",
    "LABEL.build_vocab(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        ...,\n",
       "        [-0.2082,  0.1944,  0.2650,  ...,  0.4414,  0.8425,  0.4868],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.4210, -0.5008,  0.5342,  ...,  0.6741,  0.1884,  0.3817]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The converted word embeddings are stored under TEXT.vocab.vectors\n",
    "TEXT.vocab.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'first'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The index-word correpondance is recorded in TEXT.vocab.stoi as a dictionary\n",
    "# We build a lookup dictionary for later use\n",
    "lookup = {b:a for a, b in TEXT.vocab.stoi.items()}\n",
    "lookup[30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 32: In each batch contains 32 instaces. \n",
    "#       It has been proved that training in a batch gives better performance than learn a single data at a time\n",
    "# device = torch.device(\"cuda\"): Use GPU. If you do not have a GPU enviornment, use torch.device(\"cpu\") instead\n",
    "# repeat = False: Do not repeat the iterator for multiple epochs\n",
    "# sort_key = lambda...: A key to use for sorting examples in order to batch together examples with similar lengths \n",
    "#       and minimize padding. The sort_key provided to the Iterator constructor overrides the sort_key attribute of  \n",
    "#       the Dataset, or defers to it if None\n",
    "train_iter, test_iter = data.BucketIterator.splits(\n",
    "          (train, test), batch_size=32, device=torch.device(\"cuda\"), repeat=False, sort_key=lambda x: len(x.Text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we print out the data in train_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch:\n",
      "\n",
      "[torchtext.data.batch.Batch of size 32 from TREC]\n",
      "\t[.text]:('[torch.cuda.LongTensor of size 26x32 (GPU 0)]', '[torch.cuda.LongTensor of size 32 (GPU 0)]')\n",
      "\t[.label]:[torch.cuda.LongTensor of size 32 (GPU 0)] \n",
      "\n",
      "batch.label:\n",
      "tensor([ 1,  1,  4,  4,  0,  3,  1,  0,  3,  0,  1,  3,  4,  2,\n",
      "         2,  5,  4,  1,  1,  2,  1,  0,  5,  3,  4,  4,  0,  1,\n",
      "         0,  1,  0,  1], device='cuda:0') \n",
      "\n",
      "batch.text[0]:\n",
      "tensor([[    2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2],\n",
      "        [    6,     6,     6,    29,     6,    11,    11,     6,    11,\n",
      "             6,    11,    31,     6,     6,     6,     6,     6,     6,\n",
      "             6,    31,     6,    25,     6,    11,     6,     6,     6,\n",
      "             6,     6,     6,     6,     6],\n",
      "        [    7,     7,   132,     7,   212,    28,    24,    12,    28,\n",
      "            13,    21,    21,    54,    13,   149,    21,   448,     7,\n",
      "           250,    57,     7,     5,     7,    28,  1570,  3294,   188,\n",
      "             7,     7,     7,  1125,     7],\n",
      "        [ 3097,  2816,  1871,     5,    48,  3665,  8751,     5,   316,\n",
      "           292,     5,     5,    21,     5,  7209,  3430,    43,   468,\n",
      "            42,  1305,  6415,  5157,     5,  2100,     7,     7,     8,\n",
      "          1273,     5,    10,   162,     5],\n",
      "        [  634,     9,     5,  7986,     5,    17,   167,    25,    21,\n",
      "           347,  1092,   153,   251,    25,    13,    93,     7,     4,\n",
      "          2746,    15,     4,  3333,   525,  7013,  5300,   290,     5,\n",
      "          1142,  3527,   538,    48,   244],\n",
      "        [   80,   188,    40,  1493,    96,     9,     4,     8,  5546,\n",
      "          3949,     8,  3531,  7672,     8,   121,    18,  5795,     3,\n",
      "             4,  3516,     3,    39,    25,     5,   178,     9,   150,\n",
      "             4,  6537,  8217,  1406,     8],\n",
      "        [   18,     4,    35,   178,  1476,   185,     3,  2989,    47,\n",
      "             8,  1395,    30,  2166,   281,    27,     4,    35,     1,\n",
      "             3,    16,     1,    23,     8,   158,     9,  7028,  1753,\n",
      "             3,    69,     4,  4593,  8659],\n",
      "        [    4,     3,  3563,     4,     4,   426,     1,  2647,     4,\n",
      "             4,   167,    47,     4,   546,  1172,     3,  7035,     1,\n",
      "             1,     6,     1,  2641,     5,     4,     4,    16,    20,\n",
      "             1,     8,     3,     5,  8817],\n",
      "        [    3,     1,     4,     3,     3,     4,     1,    12,     3,\n",
      "             3,   675,  1995,     3,   255,  1318,     1,     4,     1,\n",
      "             1,    65,     1,    84,  3260,     3,     3,    71,  5782,\n",
      "             1,     4,     1,  3130,    12],\n",
      "        [    1,     1,     3,     1,     1,     3,     1,   990,     1,\n",
      "             1,    42,     4,     1,    12,    16,     1,     3,     1,\n",
      "             1,   240,     1,  1319,     4,     1,     1,   172,   286,\n",
      "             1,     3,     1,    23,   978],\n",
      "        [    1,     1,     1,     1,     1,     1,     1,   392,     1,\n",
      "             1,   151,     3,     1,   359,  2334,     1,     1,     1,\n",
      "             1,  7597,     1,  4634,     3,     1,     1,     4,     4,\n",
      "             1,     1,     1,  4219,    16],\n",
      "        [    1,     1,     1,     1,     1,     1,     1,     4,     1,\n",
      "             1,   742,     1,     1,     4,    19,     1,     1,     1,\n",
      "             1,    16,     1,    49,     1,     1,     1,     3,     3,\n",
      "             1,     1,     1,     7,    23],\n",
      "        [    1,     1,     1,     1,     1,     1,     1,     3,     1,\n",
      "             1,  4063,     1,     1,     3,   558,     1,     1,     1,\n",
      "             1,    23,     1,    22,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,  8748,     5],\n",
      "        [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,  2913,     1,     1,     1,    27,     1,     1,     1,\n",
      "             1,  2215,     1,     3,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,    22,   150],\n",
      "        [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     4,     1,     1,     1,    15,     1,     1,     1,\n",
      "             1,    16,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,    15,  5943],\n",
      "        [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     3,     1,     1,     1,   765,     1,     1,     1,\n",
      "             1,    32,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,  7146,     4],\n",
      "        [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,    67,     1,     1,     1,\n",
      "             1,    47,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     4,    22],\n",
      "        [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,  1488,     1,     1,     1,\n",
      "             1,   117,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     3,     3],\n",
      "        [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,  6141,     1,     1,     1,\n",
      "             1,  3667,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1],\n",
      "        [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     4,     1,     1,     1,\n",
      "             1,  4268,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1],\n",
      "        [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     3,     1,     1,     1,\n",
      "             1,    15,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1],\n",
      "        [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,  1453,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1],\n",
      "        [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,    49,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1],\n",
      "        [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,    22,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1],\n",
      "        [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     4,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1],\n",
      "        [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     3,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1]], device='cuda:0') \n",
      "\n",
      "batch.text[0][0]:\n",
      "tensor([ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2], device='cuda:0') \n",
      "\n",
      "Fifth sentence: \n",
      "<SOS> how many yards are in 1 mile ? <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> "
     ]
    }
   ],
   "source": [
    "for num, batch in enumerate(train_iter):\n",
    "    # batch is a torchtext.data.batch.Batch object\n",
    "    # In batch there are 32 instances\n",
    "    # The data in batch have two fields: text and label\n",
    "    # The text field contains the text index itself [len(longest sentence)x32] and \n",
    "    #          the length of each setence [32] in this batch (remember include_lengths=True?)\n",
    "    # The label field contains the prediction target [32]\n",
    "    print(\"batch:\")\n",
    "    print(batch, \"\\n\")\n",
    "    \n",
    "    # Print the label\n",
    "    print(\"batch.label:\")\n",
    "    print(batch.label, \"\\n\")\n",
    "    \n",
    "    # Print the first component of batch.text (the sequence of words)\n",
    "    print(\"batch.text[0]:\")\n",
    "    print(batch.text[0], \"\\n\")\n",
    "    \n",
    "    # Print the first element in batch.text[0], which is all first words in all the sentences\n",
    "    # You may notice that the results are all 2\n",
    "    # That's because we use <SOS> to represent the beginning of a sentence\n",
    "    # Going back to TEXT.vocab.stoi you can find that the index of <SOS> is 2\n",
    "    print(\"batch.text[0][0]:\")\n",
    "    print(batch.text[0][0], \"\\n\")\n",
    "    \n",
    "    # Print the actual content fifth sentence in this batch\n",
    "    # Noted that we use the lookup dictionary to recover the word index to word\n",
    "    print(\"Fifth sentence: \")\n",
    "    for i in range(batch.text[0].size()[0]):\n",
    "        print(lookup[batch.text[0][i].tolist()[5]], end=\" \")\n",
    "    # You might find that torchtext automatically add <PAD>s after <EOS> to make all sentence \n",
    "    #           having identical length in this batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, copy the word embeddings to torch.nn.Embedding object\n",
    "Then you can use this embed for your RNN model! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        ...,\n",
       "        [-0.2082,  0.1944,  0.2650,  ...,  0.4414,  0.8425,  0.4868],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.4210, -0.5008,  0.5342,  ...,  0.6741,  0.1884,  0.3817]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The second parameter(300) is the number of dimension of your embedding model\n",
    "# padding_idx = 1: specify the <PAD>'s index is 1\n",
    "# max_norm = 1: If given, will renormalize the embeddings to always have a norm lesser than the given number\n",
    "embed = nn.Embedding(len(TEXT.vocab), 300, padding_idx=1, max_norm=1)\n",
    "\n",
    "# Copy the word vectors from dataset to nn.Embedding object\n",
    "embed.weight.data.copy_(TEXT.vocab.vectors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
